# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** Jaswanthchandu
**Raw Score:** 38/43 (88.4%)
**Course Points Earned:** 4

---

## Problem Breakdown

### Exercise 2 (9/10 = 90.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Correct t-SNE visualization: fit_transform on X_mnist_train and scatter colored by labels. Meets the task. Minor suggestions: consider tuning perplexity/learning_rate or subsampling for speed/clarity. Otherwise solid.

**Part ex1-part2** (ex1-part2.code): 2/3 points

_Feedback:_ Good attempt: you trained KNN on t-SNE features and reported performance with sensible commentary. However, you refit t-SNE on the test set (tsne.fit_transform on X_mnist_test), so train/test embeddings aren’t comparable, invalidating the accuracy. Split first and fit t-SNE on tr

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good job: you correctly used UMAP embeddings for train and transform for test, trained KNN, and computed accuracy. This matches the task and prior work. No issues noted.

---

### Exercise 4 (9/13 = 69.2%)

**Part ex2-part2** (ex2-part2.code): 6/7 points

_Feedback:_ Good use of UMAP, KNN, accuracy reporting, and visualization (2D/3D). However, you fit UMAP on the full dataset before splitting, causing data leakage. Fit UMAP on X_train and transform X_test instead. Otherwise, solid approach.

**Part ex2-part3** (ex2-part3.answer): 3/6 points

_Feedback:_ You gave a reasonable comparison, but you didn’t address the key insight: UMAP often performs slightly better in 2D with lower n_neighbors. You assert PCA is better without tying it to your runs or parameter effects. Mentioning low n_neighbors for UMAP was expected.

---

### Exercise 1 (20/20 = 100.0%)

**Part pipeline-part1** (pipeline-part1.code): 4/4 points

_Feedback:_ Correct and complete. You apply PCA to 2 components and produce a 2D scatter colored by class with a colorbar. Plot displays as required. Nice job.

**Part pipeline-part2** (pipeline-part2.code): 4/4 points

_Feedback:_ Good scree plot for first 40 components using explained_variance_ratio_. Axis labeling is fine per rubric (currently proportion labeled as percent). Consider multiplying by 100 if you want true percent, but no points deducted.

**Part pipeline-part3** (pipeline-part3.code): 4/4 points

_Feedback:_ Correct approach. You compute cumulative variance from pca_full.explained_variance_ratio_ and find the first index reaching 95%, matching prior work and the task. Good job.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Great job. You correctly used the Step 4 component count, reduced with PCA, reconstructed, and plotted the same digit (index 0). Approach and code are appropriate and should work as intended.

**Part pipeline-part5** (pipeline-part5.code): 4/4 points

_Feedback:_ Great job. You correctly applied PCA to preserve 80% variance, transformed train/test, and compared KNN performance with and without PCA using accuracy. Implementation is clear and aligned with the task.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-11-11 17:33:47 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*