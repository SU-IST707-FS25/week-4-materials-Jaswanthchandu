# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** Jaswanthchandu
**Total Score:** 29/40 (72.5%)

**Grade Category:** C- (Satisfactory)

---

## Problem Breakdown

### Exercise 1 (8/16 = 50.0%)

**Part pipeline-part1** (pipeline-part1.code): 0/0 points

_Feedback:_ Good: You applied PCA to 2D on the training set and visualized a labeled scatter plot—this meets a basic reduction/visualization goal. For a fuller “approximation,” consider inverse_transform to reconstruct images and compare to originals. Looks otherwise correct.

**Part pipeline-part2** (pipeline-part2.code): 2/4 points

_Feedback:_ You applied PCA correctly and produced a scree plot, showing understanding of variance explained. However, the task required reducing to 2 components and plotting a 2D scatter colored by class. No 2D embedding or class-colored scatter was produced.

**Part pipeline-part3** (pipeline-part3.code): 1/4 points

_Feedback:_ You computed cumulative variance and 95% threshold using pca_full, which is relevant, but you did not produce the required scree plot of the first 40 components with y-axis as percent of variance explained. Please plot explained_variance_ratio_[:40] as specified.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Good job using the previously computed n_components_95_variance to apply PCA at 95% variance and demonstrating reconstruction. This correctly leverages your prior step’s calculation. Full credit.

**Part pipeline-part5** (pipeline-part5.code): 1/4 points

_Feedback:_ You used PCA but did not complete the required visualization. You didn’t use the n_components from Step 4 (95% variance), didn’t inverse-transform a digit, and didn’t call plot_mnist_digit. The KNN accuracy work is off-task for this step. Add reconstruction and plot to earn full 

---

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Correct use of t-SNE to visualize MNIST in 2D with labels as colors. Random state set, clear scatter plot and colorbar. For potential improvements: try tuning perplexity/learning_rate or subsampling for speed. Nicely done.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job: you trained KNN on your t-SNE embedding and reported performance, addressing the question. Note: you re-fit t-SNE on the test set; for fair comparison, fit t-SNE once (e.g., on full data) before KNN. Otherwise solid implementation and insight.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Correct approach: you fit KNN on the UMAP-transformed train set and evaluated on UMAP-transformed test set using prior variables. Assumes imports exist, which is fine. Nice job. You could tune n_neighbors for better accuracy, but this meets the task.

---

### Exercise 4 (11/14 = 78.6%)

**Part ex2-part2** (ex2-part2.code): 5/7 points

_Feedback:_ Good alternative approach using UMAP, KNN, and visualization. However, you fit UMAP on the full data before the train/test split, causing data leakage. Split first, fit UMAP on X_train, then transform X_train/X_test. The exercise asked for PCA—consider PCA(.9) as requested.

**Part ex2-part3** (ex2-part3.answer): 6/7 points

_Feedback:_ Good grasp: UMAP forms clear clusters; KNN accuracy depends on n_neighbors/min_dist. However, you didn’t reference concrete results from your run (e.g., 2D vs 3D accuracies) and the claim that PCA classifies better isn’t supported by the UMAP results you discussed.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-10-27 18:51:11 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*